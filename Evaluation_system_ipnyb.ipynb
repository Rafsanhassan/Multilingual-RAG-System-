{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8dntggeUpT3"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Import our RAG system\n",
        "from multilingual_rag import MultilingualRAG, DocumentChunk\n",
        "\n",
        "@dataclass\n",
        "class EvaluationResult:\n",
        "    \"\"\"Stores evaluation results for a single query\"\"\"\n",
        "    query: str\n",
        "    expected_answer: str\n",
        "    generated_answer: str\n",
        "    retrieved_chunks: List[Tuple[DocumentChunk, float]]\n",
        "    groundedness_score: float\n",
        "    relevance_score: float\n",
        "    answer_similarity: float\n",
        "    retrieval_precision: float\n",
        "    response_time: float\n",
        "    language: str\n",
        "\n",
        "class RAGEvaluator:\n",
        "    \"\"\"Comprehensive RAG evaluation system\"\"\"\n",
        "\n",
        "    def __init__(self, rag_system: MultilingualRAG):\n",
        "        self.rag = rag_system\n",
        "        self.similarity_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "        self.evaluation_results: List[EvaluationResult] = []\n",
        "\n",
        "    def load_test_dataset(self, dataset_path: str = None) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Load test dataset with ground truth answers\"\"\"\n",
        "\n",
        "        # Default test cases based on your requirements\n",
        "        default_test_cases = [\n",
        "            {\n",
        "                \"query\": \"অনুপমের ভাষায় সুপুরুষ কাকে বলা হয়েছে?\",\n",
        "                \"expected_answer\": \"শুম্ভুনাথ\",\n",
        "                \"category\": \"character_identification\",\n",
        "                \"language\": \"bn\"\n",
        "            },\n",
        "            {\n",
        "                \"query\": \"কাকে অনুপমের ভাগ্য দেবতা বলে উল্লেখ করা হয়েছে?\",\n",
        "                \"expected_answer\": \"মামাকে\",\n",
        "                \"category\": \"character_relationship\",\n",
        "                \"language\": \"bn\"\n",
        "            },\n",
        "            {\n",
        "                \"query\": \"বিয়ের সময় কল্যাণীর প্রকৃত বয়স কত ছিল?\",\n",
        "                \"expected_answer\": \"১৫ বছর\",\n",
        "                \"category\": \"factual_information\",\n",
        "                \"language\": \"bn\"\n",
        "            },\n",
        "            {\n",
        "                \"query\": \"Who is described as a handsome man in Anupam's words?\",\n",
        "                \"expected_answer\": \"Shumbhunath\",\n",
        "                \"category\": \"character_identification\",\n",
        "                \"language\": \"en\"\n",
        "            },\n",
        "            {\n",
        "                \"query\": \"What was Kalyani's actual age at the time of marriage?\",\n",
        "                \"expected_answer\": \"15 years\",\n",
        "                \"category\": \"factual_information\",\n",
        "                \"language\": \"en\"\n",
        "            },\n",
        "            {\n",
        "                \"query\": \"অনুপম কার সাথে বিয়ে করেছিল?\",\n",
        "                \"expected_answer\": \"কল্যাণী\",\n",
        "                \"category\": \"plot_detail\",\n",
        "                \"language\": \"bn\"\n",
        "            },\n",
        "            {\n",
        "                \"query\": \"গল্পের মূল চরিত্র কে?\",\n",
        "                \"expected_answer\": \"অনুপম\",\n",
        "                \"category\": \"main_character\",\n",
        "                \"language\": \"bn\"\n",
        "            },\n",
        "            {\n",
        "                \"query\": \"অনুপমের মামা কী ধরনের মানুষ ছিলেন?\",\n",
        "                \"expected_answer\": \"প্রভাবশালী এবং বিত্তবান\",\n",
        "                \"category\": \"character_description\",\n",
        "                \"language\": \"bn\"\n",
        "            },\n",
        "            {\n",
        "                \"query\": \"কল্যাণীর বিয়েতে কোন সমস্যা হয়েছিল?\",\n",
        "                \"expected_answer\": \"বয়সের মিথ্যা তথ্য\",\n",
        "                \"category\": \"plot_conflict\",\n",
        "                \"language\": \"bn\"\n",
        "            },\n",
        "            {\n",
        "                \"query\": \"What is the main conflict in the story?\",\n",
        "                \"expected_answer\": \"Age deception in marriage\",\n",
        "                \"category\": \"plot_conflict\",\n",
        "                \"language\": \"en\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        if dataset_path:\n",
        "            try:\n",
        "                with open(dataset_path, 'r', encoding='utf-8') as f:\n",
        "                    return json.load(f)\n",
        "            except FileNotFoundError:\n",
        "                print(f\"Dataset file {dataset_path} not found. Using default test cases.\")\n",
        "\n",
        "        return default_test_cases\n",
        "\n",
        "    def evaluate_groundedness(self, answer: str, retrieved_chunks: List[Tuple[DocumentChunk, float]]) -> float:\n",
        "        \"\"\"\n",
        "        Evaluate if the answer is grounded in the retrieved context\n",
        "        Returns score between 0 and 1\n",
        "        \"\"\"\n",
        "        if not retrieved_chunks or not answer.strip():\n",
        "            return 0.0\n",
        "\n",
        "        # Combine all retrieved context\n",
        "        context = \" \".join([chunk.text for chunk, _ in retrieved_chunks])\n",
        "\n",
        "        # Encode answer and context\n",
        "        answer_embedding = self.similarity_model.encode([answer])\n",
        "        context_embedding = self.similarity_model.encode([context])\n",
        "\n",
        "        # Calculate similarity\n",
        "        similarity = cosine_similarity(answer_embedding, context_embedding)[0][0]\n",
        "\n",
        "        # Additional checks for direct content overlap\n",
        "        answer_words = set(answer.lower().split())\n",
        "        context_words = set(context.lower().split())\n",
        "\n",
        "        # Word overlap ratio\n",
        "        if len(answer_words) > 0:\n",
        "            word_overlap = len(answer_words.intersection(context_words)) / len(answer_words)\n",
        "        else:\n",
        "            word_overlap = 0.0\n",
        "\n",
        "        # Combine semantic similarity and word overlap\n",
        "        groundedness_score = 0.7 * similarity + 0.3 * word_overlap\n",
        "\n",
        "        return min(groundedness_score, 1.0)\n",
        "\n",
        "    def evaluate_relevance(self, query: str, retrieved_chunks: List[Tuple[DocumentChunk, float]]) -> float:\n",
        "        \"\"\"\n",
        "        Evaluate if retrieved documents are relevant to the query\n",
        "        Returns average relevance score\n",
        "        \"\"\"\n",
        "        if not retrieved_chunks:\n",
        "            return 0.0\n",
        "\n",
        "        query_embedding = self.similarity_model.encode([query])\n",
        "        relevance_scores = []\n",
        "\n",
        "        for chunk, retrieval_score in retrieved_chunks:\n",
        "            # Use the retrieval score as primary relevance indicator\n",
        "            relevance_scores.append(retrieval_score)\n",
        "\n",
        "        return np.mean(relevance_scores)\n",
        "\n",
        "    def evaluate_answer_similarity(self, expected_answer: str, generated_answer: str) -> float:\n",
        "        \"\"\"\n",
        "        Evaluate similarity between expected and generated answers\n",
        "        \"\"\"\n",
        "        if not expected_answer.strip() or not generated_answer.strip():\n",
        "            return 0.0\n",
        "\n",
        "        # Exact match check (case insensitive)\n",
        "        if expected_answer.lower().strip() in generated_answer.lower().strip():\n",
        "            return 1.0\n",
        "\n",
        "        # Semantic similarity\n",
        "        expected_embedding = self.similarity_model.encode([expected_answer])\n",
        "        generated_embedding = self.similarity_model.encode([generated_answer])\n",
        "\n",
        "        similarity = cosine_similarity(expected_embedding, generated_embedding)[0][0]\n",
        "\n",
        "        return similarity\n",
        "\n",
        "    def evaluate_retrieval_precision(self, query: str, retrieved_chunks: List[Tuple[DocumentChunk, float]],\n",
        "                                   expected_answer: str, threshold: float = 0.3) -> float:\n",
        "        \"\"\"\n",
        "        Evaluate precision of retrieval (how many retrieved chunks are actually relevant)\n",
        "        \"\"\"\n",
        "        if not retrieved_chunks:\n",
        "            return 0.0\n",
        "\n",
        "        relevant_count = 0\n",
        "        expected_embedding = self.similarity_model.encode([expected_answer])\n",
        "\n",
        "        for chunk, score in retrieved_chunks:\n",
        "            chunk_embedding = self.similarity_model.encode([chunk.text])\n",
        "            chunk_relevance = cosine_similarity(expected_embedding, chunk_embedding)[0][0]\n",
        "\n",
        "            if chunk_relevance >= threshold or score >= threshold:\n",
        "                relevant_count += 1\n",
        "\n",
        "        return relevant_count / len(retrieved_chunks)\n",
        "\n",
        "    def evaluate_single_query(self, test_case: Dict[str, Any]) -> EvaluationResult:\n",
        "        \"\"\"Evaluate a single query\"\"\"\n",
        "\n",
        "        query = test_case[\"query\"]\n",
        "        expected_answer = test_case[\"expected_answer\"]\n",
        "\n",
        "        # Measure response time\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        # Retrieve chunks\n",
        "        retrieved_chunks = self.rag.retrieve_relevant_chunks(query, k=5)\n",
        "\n",
        "        # Generate answer\n",
        "        generated_answer = self.rag.chat(query)\n",
        "\n",
        "        end_time = datetime.now()\n",
        "        response_time = (end_time - start_time).total_seconds()\n",
        "\n",
        "        # Calculate metrics\n",
        "        groundedness_score = self.evaluate_groundedness(generated_answer, retrieved_chunks)\n",
        "        relevance_score = self.evaluate_relevance(query, retrieved_chunks)\n",
        "        answer_similarity = self.evaluate_answer_similarity(expected_answer, generated_answer)\n",
        "        retrieval_precision = self.evaluate_retrieval_precision(query, retrieved_chunks, expected_answer)\n",
        "\n",
        "        return EvaluationResult(\n",
        "            query=query,\n",
        "            expected_answer=expected_answer,\n",
        "            generated_answer=generated_answer,\n",
        "            retrieved_chunks=retrieved_chunks,\n",
        "            groundedness_score=groundedness_score,\n",
        "            relevance_score=relevance_score,\n",
        "            answer_similarity=answer_similarity,\n",
        "            retrieval_precision=retrieval_precision,\n",
        "            response_time=response_time,\n",
        "            language=test_case.get(\"language\", \"unknown\")\n",
        "        )\n",
        "\n",
        "    def run_evaluation(self, dataset_path: str = None) -> Dict[str, Any]:\n",
        "        \"\"\"Run complete evaluation on test dataset\"\"\"\n",
        "\n",
        "        print(\"Starting RAG System Evaluation...\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Load test dataset\n",
        "        test_cases = self.load_test_dataset(dataset_path)\n",
        "\n",
        "        # Evaluate each test case\n",
        "        results = []\n",
        "        for i, test_case in enumerate(test_cases, 1):\n",
        "            print(f\"Evaluating {i}/{len(test_cases)}: {test_case['query'][:50]}...\")\n",
        "\n",
        "            result = self.evaluate_single_query(test_case)\n",
        "            results.append(result)\n",
        "\n",
        "            # Print individual result\n",
        "            print(f\"  Expected: {result.expected_answer}\")\n",
        "            print(f\"  Generated: {result.generated_answer}\")\n",
        "            print(f\"  Answer Similarity: {result.answer_similarity:.3f}\")\n",
        "            print(f\"  Groundedness: {result.groundedness_score:.3f}\")\n",
        "            print(f\"  Relevance: {result.relevance_score:.3f}\")\n",
        "            print(f\"  Response Time: {result.response_time:.3f}s\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "        self.evaluation_results = results\n",
        "\n",
        "        # Calculate aggregate metrics\n",
        "        aggregate_metrics = self.calculate_aggregate_metrics(results)\n",
        "\n",
        "        # Generate detailed report\n",
        "        report = self.generate_evaluation_report(results, aggregate_metrics)\n",
        "\n",
        "        return report\n",
        "\n",
        "    def calculate_aggregate_metrics(self, results: List[EvaluationResult]) -> Dict[str, float]:\n",
        "        \"\"\"Calculate aggregate metrics across all test cases\"\"\"\n",
        "\n",
        "        if not results:\n",
        "            return {}\n",
        "\n",
        "        metrics = {\n",
        "            \"avg_groundedness\": np.mean([r.groundedness_score for r in results]),\n",
        "            \"avg_relevance\": np.mean([r.relevance_score for r in results]),\n",
        "            \"avg_answer_similarity\": np.mean([r.answer_similarity for r in results]),\n",
        "            \"avg_retrieval_precision\": np.mean([r.retrieval_precision for r in results]),\n",
        "            \"avg_response_time\": np.mean([r.response_time for r in results]),\n",
        "            \"exact_match_rate\": sum(1 for r in results if r.answer_similarity >= 0.9) / len(results),\n",
        "            \"good_groundedness_rate\": sum(1 for r in results if r.groundedness_score >= 0.7) / len(results),\n",
        "            \"good_relevance_rate\": sum(1 for r in results if r.relevance_score >= 0.5) / len(results)\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def generate_evaluation_report(self, results: List[EvaluationResult],\n",
        "                                 aggregate_metrics: Dict[str, float]) -> Dict[str, Any]:\n",
        "        \"\"\"Generate comprehensive evaluation report\"\"\"\n",
        "\n",
        "        report = {\n",
        "            \"evaluation_summary\": {\n",
        "                \"total_test_cases\": len(results),\n",
        "                \"evaluation_date\": datetime.now().isoformat(),\n",
        "                \"aggregate_metrics\": aggregate_metrics\n",
        "            },\n",
        "            \"detailed_results\": [],\n",
        "            \"language_breakdown\": {},\n",
        "            \"performance_analysis\": {}\n",
        "        }\n",
        "\n",
        "        # Detailed results\n",
        "        for result in results:\n",
        "            report[\"detailed_results\"].append({\n",
        "                \"query\": result.query,\n",
        "                \"expected_answer\": result.expected_answer,\n",
        "                \"generated_answer\": result.generated_answer,\n",
        "                \"metrics\": {\n",
        "                    \"groundedness_score\": result.groundedness_score,\n",
        "                    \"relevance_score\": result.relevance_score,\n",
        "                    \"answer_similarity\": result.answer_similarity,\n",
        "                    \"retrieval_precision\": result.retrieval_precision,\n",
        "                    \"response_time\": result.response_time\n",
        "                },\n",
        "                \"language\": result.language,\n",
        "                \"retrieved_chunks_count\": len(result.retrieved_chunks)\n",
        "            })\n",
        "\n",
        "        # Language breakdown\n",
        "        language_groups = {}\n",
        "        for result in results:\n",
        "            lang = result.language\n",
        "            if lang not in language_groups:\n",
        "                language_groups[lang] = []\n",
        "            language_groups[lang].append(result)\n",
        "\n",
        "        for lang, lang_results in language_groups.items():\n",
        "            report[\"language_breakdown\"][lang] = {\n",
        "                \"count\": len(lang_results),\n",
        "                \"avg_groundedness\": np.mean([r.groundedness_score for r in lang_results]),\n",
        "                \"avg_relevance\": np.mean([r.relevance_score for r in lang_results]),\n",
        "                \"avg_answer_similarity\": np.mean([r.answer_similarity for r in lang_results]),\n",
        "                \"avg_response_time\": np.mean([r.response_time for r in lang_results])\n",
        "            }\n",
        "\n",
        "        # Performance analysis\n",
        "        report[\"performance_analysis\"] = {\n",
        "            \"best_performing_queries\": sorted(\n",
        "                [(r.query, r.answer_similarity) for r in results],\n",
        "                key=lambda x: x[1], reverse=True\n",
        "            )[:3],\n",
        "            \"worst_performing_queries\": sorted(\n",
        "                [(r.query, r.answer_similarity) for r in results],\n",
        "                key=lambda x: x[1]\n",
        "            )[:3],\n",
        "            \"response_time_distribution\": {\n",
        "                \"min\": min(r.response_time for r in results),\n",
        "                \"max\": max(r.response_time for r in results),\n",
        "                \"median\": np.median([r.response_time for r in results])\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return report\n",
        "\n",
        "    def save_evaluation_report(self, report: Dict[str, Any], filename: str = None):\n",
        "        \"\"\"Save evaluation report to JSON file\"\"\"\n",
        "\n",
        "        if filename is None:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            filename = f\"rag_evaluation_report_{timestamp}.json\"\n",
        "\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(report, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"Evaluation report saved to {filename}\")\n",
        "\n",
        "    def visualize_results(self, save_plots: bool = True):\n",
        "        \"\"\"Generate visualization plots for evaluation results\"\"\"\n",
        "\n",
        "        if not self.evaluation_results:\n",
        "            print(\"No evaluation results to visualize. Run evaluation first.\")\n",
        "            return\n",
        "\n",
        "        # Create subplots\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        fig.suptitle('RAG System Evaluation Results', fontsize=16)\n",
        "\n",
        "        # Extract data\n",
        "        results = self.evaluation_results\n",
        "        metrics = ['groundedness_score', 'relevance_score', 'answer_similarity',\n",
        "                  'retrieval_precision', 'response_time']\n",
        "\n",
        "        # 1. Overall metrics bar chart\n",
        "        avg_scores = [np.mean([getattr(r, metric) for r in results]) for metric in metrics]\n",
        "        axes[0, 0].bar(metrics, avg_scores, color=['skyblue', 'lightgreen', 'salmon', 'gold', 'plum'])\n",
        "        axes[0, 0].set_title('Average Metrics')\n",
        "        axes[0, 0].set_ylabel('Score')\n",
        "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # 2. Language comparison\n",
        "        languages = list(set(r.language for r in results))\n",
        "        if len(languages) > 1:\n",
        "            lang_data = {}\n",
        "            for lang in languages:\n",
        "                lang_results = [r for r in results if r.language == lang]\n",
        "                lang_data[lang] = np.mean([r.answer_similarity for r in lang_results])\n",
        "\n",
        "            axes[0, 1].bar(lang_data.keys(), lang_data.values(), color=['coral', 'lightblue'])\n",
        "            axes[0, 1].set_title('Answer Similarity by Language')\n",
        "            axes[0, 1].set_ylabel('Average Similarity Score')\n",
        "\n",
        "        # 3. Response time distribution\n",
        "        response_times = [r.response_time for r in results]\n",
        "        axes[0, 2].hist(response_times, bins=10, color='lightgreen', alpha=0.7)\n",
        "        axes[0, 2].set_title('Response Time Distribution')\n",
        "        axes[0, 2].set_xlabel('Response Time (seconds)')\n",
        "        axes[0, 2].set_ylabel('Frequency')\n",
        "\n",
        "        # 4. Groundedness vs Relevance scatter\n",
        "        groundedness = [r.groundedness_score for r in results]\n",
        "        relevance = [r.relevance_score for r in results]\n",
        "        axes[1, 0].scatter(groundedness, relevance, alpha=0.6, color='purple')\n",
        "        axes[1, 0].set_xlabel('Groundedness Score')\n",
        "        axes[1, 0].set_ylabel('Relevance Score')\n",
        "        axes[1, 0].set_title('Groundedness vs Relevance')\n",
        "\n",
        "        # 5. Answer similarity distribution\n",
        "        similarities = [r.answer_similarity for r in results]\n",
        "        axes[1, 1].hist(similarities, bins=10, color='orange', alpha=0.7)\n",
        "        axes[1, 1].set_title('Answer Similarity Distribution')\n",
        "        axes[1, 1].set_xlabel('Similarity Score')\n",
        "        axes[1, 1].set_ylabel('Frequency')\n",
        "\n",
        "        # 6. Performance by query category (if available)\n",
        "        # This requires category information in test cases\n",
        "        categories = {}\n",
        "        for i, result in enumerate(results):\n",
        "            # Try to extract category from test case if available\n",
        "            category = \"General\"  # Default category\n",
        "            categories[category] = categories.get(category, []) + [result.answer_similarity]\n",
        "\n",
        "        if len(categories) > 1:\n",
        "            cat_names = list(categories.keys())\n",
        "            cat_scores = [np.mean(scores) for scores in categories.values()]\n",
        "            axes[1, 2].bar(cat_names, cat_scores, color='lightcoral')\n",
        "            axes[1, 2].set_title('Performance by Category')\n",
        "            axes[1, 2].set_ylabel('Average Similarity Score')\n",
        "            axes[1, 2].tick_params(axis='x', rotation=45)\n",
        "        else:\n",
        "            # Show overall performance summary\n",
        "            summary_data = {\n",
        "                'Excellent (>0.9)': sum(1 for s in similarities if s > 0.9),\n",
        "                'Good (0.7-0.9)': sum(1 for s in similarities if 0.7 <= s <= 0.9),\n",
        "                'Fair (0.5-0.7)': sum(1 for s in similarities if 0.5 <= s < 0.7),\n",
        "                'Poor (<0.5)': sum(1 for s in similarities if s < 0.5)\n",
        "            }\n",
        "            axes[1, 2].pie(summary_data.values(), labels=summary_data.keys(), autopct='%1.1f%%')\n",
        "            axes[1, 2].set_title('Performance Distribution')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_plots:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            plt.savefig(f\"rag_evaluation_plots_{timestamp}.png\", dpi=300, bbox_inches='tight')\n",
        "            print(f\"Plots saved to rag_evaluation_plots_{timestamp}.png\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def generate_human_evaluation_template(self, sample_size: int = 5) -> Dict[str, Any]:\n",
        "        \"\"\"Generate template for human evaluation\"\"\"\n",
        "\n",
        "        if not self.evaluation_results:\n",
        "            print(\"No evaluation results available. Run evaluation first.\")\n",
        "            return {}\n",
        "\n",
        "        # Select sample for human evaluation\n",
        "        sample_results = np.random.choice(self.evaluation_results,\n",
        "                                        min(sample_size, len(self.evaluation_results)),\n",
        "                                        replace=False)\n",
        "\n",
        "        template = {\n",
        "            \"instructions\": {\n",
        "                \"groundedness\": \"Rate how well the answer is supported by the provided context (1-5 scale)\",\n",
        "                \"relevance\": \"Rate how relevant the retrieved information is to the question (1-5 scale)\",\n",
        "                \"accuracy\": \"Rate the factual accuracy of the answer (1-5 scale)\",\n",
        "                \"fluency\": \"Rate the language fluency and readability (1-5 scale)\"\n",
        "            },\n",
        "            \"evaluation_samples\": []\n",
        "        }\n",
        "\n",
        "        for i, result in enumerate(sample_results):\n",
        "            context = \"\\n\".join([chunk.text[:200] + \"...\" for chunk, _ in result.retrieved_chunks[:3]])\n",
        "\n",
        "            template[\"evaluation_samples\"].append({\n",
        "                \"sample_id\": i + 1,\n",
        "                \"query\": result.query,\n",
        "                \"generated_answer\": result.generated_answer,\n",
        "                \"expected_answer\": result.expected_answer,\n",
        "                \"retrieved_context\": context,\n",
        "                \"system_scores\": {\n",
        "                    \"groundedness\": result.groundedness_score,\n",
        "                    \"relevance\": result.relevance_score,\n",
        "                    \"answer_similarity\": result.answer_similarity\n",
        "                },\n",
        "                \"human_evaluation\": {\n",
        "                    \"groundedness\": None,\n",
        "                    \"relevance\": None,\n",
        "                    \"accuracy\": None,\n",
        "                    \"fluency\": None,\n",
        "                    \"comments\": \"\"\n",
        "                }\n",
        "            })\n",
        "\n",
        "        return template\n",
        "\n",
        "    def compare_with_baseline(self, baseline_results: List[EvaluationResult]) -> Dict[str, Any]:\n",
        "        \"\"\"Compare current results with baseline results\"\"\"\n",
        "\n",
        "        if not self.evaluation_results or not baseline_results:\n",
        "            return {\"error\": \"Missing evaluation results for comparison\"}\n",
        "\n",
        "        current_metrics = self.calculate_aggregate_metrics(self.evaluation_results)\n",
        "        baseline_metrics = self.calculate_aggregate_metrics(baseline_results)\n",
        "\n",
        "        comparison = {\n",
        "            \"current_performance\": current_metrics,\n",
        "            \"baseline_performance\": baseline_metrics,\n",
        "            \"improvements\": {},\n",
        "            \"regressions\": {}\n",
        "        }\n",
        "\n",
        "        for metric in current_metrics:\n",
        "            if metric in baseline_metrics:\n",
        "                diff = current_metrics[metric] - baseline_metrics[metric]\n",
        "                if diff > 0:\n",
        "                    comparison[\"improvements\"][metric] = {\n",
        "                        \"improvement\": diff,\n",
        "                        \"percentage_change\": (diff / baseline_metrics[metric]) * 100\n",
        "                    }\n",
        "                elif diff < 0:\n",
        "                    comparison[\"regressions\"][metric] = {\n",
        "                        \"regression\": abs(diff),\n",
        "                        \"percentage_change\": (diff / baseline_metrics[metric]) * 100\n",
        "                    }\n",
        "\n",
        "        return comparison\n",
        "\n",
        "class BenchmarkRunner:\n",
        "    \"\"\"Run standardized benchmarks for RAG systems\"\"\"\n",
        "\n",
        "    def __init__(self, rag_system: MultilingualRAG):\n",
        "        self.rag = rag_system\n",
        "        self.evaluator = RAGEvaluator(rag_system)\n",
        "\n",
        "    def run_multilingual_benchmark(self) -> Dict[str, Any]:\n",
        "        \"\"\"Run comprehensive multilingual benchmark\"\"\"\n",
        "\n",
        "        print(\"Running Multilingual RAG Benchmark...\")\n",
        "\n",
        "        # Bengali literature specific test cases\n",
        "        bengali_tests = [\n",
        "            {\n",
        "                \"query\": \"রবীন্দ্রনাথ ঠাকুরের কোন উপন্যাসে গোরা চরিত্রটি আছে?\",\n",
        "                \"expected_answer\": \"গোরা\",\n",
        "                \"category\": \"literature_knowledge\",\n",
        "                \"language\": \"bn\"\n",
        "            },\n",
        "            {\n",
        "                \"query\": \"বাংলা সাহিত্যের আধুনিক যুগের প্রবর্তক কে?\",\n",
        "                \"expected_answer\": \"মাইকেল মধুসূদন দত্ত\",\n",
        "                \"category\": \"literature_history\",\n",
        "                \"language\": \"bn\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # English equivalent tests\n",
        "        english_tests = [\n",
        "            {\n",
        "                \"query\": \"Who is considered the pioneer of modern Bengali literature?\",\n",
        "                \"expected_answer\": \"Michael Madhusudan Dutt\",\n",
        "                \"category\": \"literature_history\",\n",
        "                \"language\": \"en\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Combine all tests\n",
        "        all_tests = bengali_tests + english_tests\n",
        "\n",
        "        # Run evaluation\n",
        "        results = []\n",
        "        for test in all_tests:\n",
        "            result = self.evaluator.evaluate_single_query(test)\n",
        "            results.append(result)\n",
        "\n",
        "        # Calculate metrics\n",
        "        benchmark_report = {\n",
        "            \"benchmark_name\": \"Multilingual Bengali Literature RAG\",\n",
        "            \"version\": \"1.0\",\n",
        "            \"results\": results,\n",
        "            \"aggregate_metrics\": self.evaluator.calculate_aggregate_metrics(results),\n",
        "            \"language_specific_performance\": {}\n",
        "        }\n",
        "\n",
        "        # Language-specific analysis\n",
        "        for lang in ['bn', 'en']:\n",
        "            lang_results = [r for r in results if r.language == lang]\n",
        "            if lang_results:\n",
        "                benchmark_report[\"language_specific_performance\"][lang] = \\\n",
        "                    self.evaluator.calculate_aggregate_metrics(lang_results)\n",
        "\n",
        "        return benchmark_report\n",
        "\n",
        "# Example usage and testing\n",
        "def main():\n",
        "    \"\"\"Example usage of the RAG evaluation system\"\"\"\n",
        "\n",
        "    # Initialize RAG system (make sure it's built with knowledge base)\n",
        "    rag = MultilingualRAG()\n",
        "\n",
        "    # Initialize evaluator\n",
        "    evaluator = RAGEvaluator(rag)\n",
        "\n",
        "    # Run evaluation\n",
        "    print(\"Running RAG System Evaluation...\")\n",
        "    evaluation_report = evaluator.run_evaluation()\n",
        "\n",
        "    # Save report\n",
        "    evaluator.save_evaluation_report(evaluation_report)\n",
        "\n",
        "    # Generate visualizations\n",
        "    evaluator.visualize_results()\n",
        "\n",
        "    # Generate human evaluation template\n",
        "    human_eval_template = evaluator.generate_human_evaluation_template(sample_size=3)\n",
        "\n",
        "    with open(\"human_evaluation_template.json\", 'w', encoding='utf-8') as f:\n",
        "        json.dump(human_eval_template, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(\"Human evaluation template saved to human_evaluation_template.json\")\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"EVALUATION SUMMARY\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    metrics = evaluation_report[\"evaluation_summary\"][\"aggregate_metrics\"]\n",
        "    print(f\"Average Groundedness: {metrics['avg_groundedness']:.3f}\")\n",
        "    print(f\"Average Relevance: {metrics['avg_relevance']:.3f}\")\n",
        "    print(f\"Average Answer Similarity: {metrics['avg_answer_similarity']:.3f}\")\n",
        "    print(f\"Average Response Time: {metrics['avg_response_time']:.3f}s\")\n",
        "    print(f\"Exact Match Rate: {metrics['exact_match_rate']:.3f}\")\n",
        "\n",
        "    # Run benchmark\n",
        "    benchmark_runner = BenchmarkRunner(rag)\n",
        "    benchmark_results = benchmark_runner.run_multilingual_benchmark()\n",
        "\n",
        "    print(\"\\nBenchmark completed!\")\n",
        "    return evaluation_report, benchmark_results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}