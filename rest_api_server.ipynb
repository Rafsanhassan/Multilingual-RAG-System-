{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any\n",
        "import asyncio\n",
        "import uuid\n",
        "\n",
        "# Import our RAG system\n",
        "from multilingual_rag import MultilingualRAG, DocumentChunk\n",
        "\n",
        "# For advanced LLM integration\n",
        "import openai\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.llms import Ollama\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "# For database integration\n",
        "import psycopg2\n",
        "from pymongo import MongoClient\n",
        "import pinecone"
      ],
      "metadata": {
        "id": "GjYiE-5STRIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Da4sAUvfTL-n"
      },
      "outputs": [],
      "source": [
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class AdvancedRAGAPI:\n",
        "    \"\"\"Advanced RAG API with multiple LLM providers and vector databases\"\"\"\n",
        "\n",
        "    def __init__(self, config_path: str = \"rag_config.json\"):\n",
        "        self.app = Flask(__name__)\n",
        "        CORS(self.app)\n",
        "\n",
        "        # Load configuration\n",
        "        self.config = self.load_config(config_path)\n",
        "\n",
        "        # Initialize RAG system\n",
        "        self.rag = MultilingualRAG(\n",
        "            vector_store_path=self.config.get(\"vector_store_path\", \"vectorstore\")\n",
        "        )\n",
        "\n",
        "        # Initialize LLM providers\n",
        "        self.llm_providers = self.initialize_llm_providers()\n",
        "\n",
        "        # Initialize vector database\n",
        "        self.vector_db = self.initialize_vector_database()\n",
        "\n",
        "        # Session management\n",
        "        self.sessions = {}\n",
        "\n",
        "        # Setup routes\n",
        "        self.setup_routes()\n",
        "\n",
        "    def load_config(self, config_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Load configuration from JSON file\"\"\"\n",
        "        default_config = {\n",
        "            \"llm_providers\": {\n",
        "                \"openai\": {\"api_key\": \"\", \"model\": \"gpt-3.5-turbo\"},\n",
        "                \"google\": {\"api_key\": \"\", \"model\": \"gemini-pro\"},\n",
        "                \"ollama\": {\"base_url\": \"http://localhost:11434\", \"model\": \"llama2\"}\n",
        "            },\n",
        "            \"vector_database\": {\n",
        "                \"type\": \"faiss\",  # faiss, pinecone, postgres, mongodb\n",
        "                \"connection_params\": {}\n",
        "            },\n",
        "            \"retrieval_params\": {\n",
        "                \"top_k\": 5,\n",
        "                \"similarity_threshold\": 0.3\n",
        "            },\n",
        "            \"generation_params\": {\n",
        "                \"max_tokens\": 300,\n",
        "                \"temperature\": 0.1\n",
        "            }\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            with open(config_path, 'r', encoding='utf-8') as f:\n",
        "                config = json.load(f)\n",
        "            return {**default_config, **config}\n",
        "        except FileNotFoundError:\n",
        "            logger.warning(f\"Config file {config_path} not found. Using default config.\")\n",
        "            return default_config\n",
        "\n",
        "    def initialize_llm_providers(self) -> Dict[str, Any]:\n",
        "        \"\"\"Initialize different LLM providers\"\"\"\n",
        "        providers = {}\n",
        "\n",
        "        # OpenAI\n",
        "        if self.config[\"llm_providers\"][\"openai\"][\"api_key\"]:\n",
        "            try:\n",
        "                openai.api_key = self.config[\"llm_providers\"][\"openai\"][\"api_key\"]\n",
        "                providers[\"openai\"] = ChatOpenAI(\n",
        "                    api_key=self.config[\"llm_providers\"][\"openai\"][\"api_key\"],\n",
        "                    model=self.config[\"llm_providers\"][\"openai\"][\"model\"],\n",
        "                    temperature=self.config[\"generation_params\"][\"temperature\"]\n",
        "                )\n",
        "                logger.info(\"OpenAI provider initialized\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Failed to initialize OpenAI: {e}\")\n",
        "\n",
        "        # Google Gemini\n",
        "        if self.config[\"llm_providers\"][\"google\"][\"api_key\"]:\n",
        "            try:\n",
        "                providers[\"google\"] = ChatGoogleGenerativeAI(\n",
        "                    google_api_key=self.config[\"llm_providers\"][\"google\"][\"api_key\"],\n",
        "                    model=self.config[\"llm_providers\"][\"google\"][\"model\"],\n",
        "                    temperature=self.config[\"generation_params\"][\"temperature\"]\n",
        "                )\n",
        "                logger.info(\"Google Gemini provider initialized\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Failed to initialize Google Gemini: {e}\")\n",
        "\n",
        "        # Ollama (local)\n",
        "        try:\n",
        "            providers[\"ollama\"] = Ollama(\n",
        "                base_url=self.config[\"llm_providers\"][\"ollama\"][\"base_url\"],\n",
        "                model=self.config[\"llm_providers\"][\"ollama\"][\"model\"]\n",
        "            )\n",
        "            logger.info(\"Ollama provider initialized\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to initialize Ollama: {e}\")\n",
        "\n",
        "        return providers\n",
        "\n",
        "    def initialize_vector_database(self):\n",
        "        \"\"\"Initialize vector database based on configuration\"\"\"\n",
        "        db_type = self.config[\"vector_database\"][\"type\"]\n",
        "\n",
        "        if db_type == \"pinecone\":\n",
        "            return self.setup_pinecone()\n",
        "        elif db_type == \"postgres\":\n",
        "            return self.setup_postgres()\n",
        "        elif db_type == \"mongodb\":\n",
        "            return self.setup_mongodb()\n",
        "        else:\n",
        "            # Default to FAISS (already in RAG system)\n",
        "            return None\n",
        "\n",
        "    def setup_pinecone(self):\n",
        "        \"\"\"Setup Pinecone vector database\"\"\"\n",
        "        try:\n",
        "            api_key = self.config[\"vector_database\"][\"connection_params\"].get(\"api_key\")\n",
        "            if api_key:\n",
        "                pinecone.init(api_key=api_key)\n",
        "                index_name = self.config[\"vector_database\"][\"connection_params\"].get(\"index_name\", \"rag-index\")\n",
        "                return pinecone.Index(index_name)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to setup Pinecone: {e}\")\n",
        "        return None\n",
        "\n",
        "    def setup_postgres(self):\n",
        "        \"\"\"Setup PostgreSQL with pgvector\"\"\"\n",
        "        try:\n",
        "            params = self.config[\"vector_database\"][\"connection_params\"]\n",
        "            conn = psycopg2.connect(\n",
        "                host=params.get(\"host\", \"localhost\"),\n",
        "                database=params.get(\"database\", \"rag_db\"),\n",
        "                user=params.get(\"user\", \"postgres\"),\n",
        "                password=params.get(\"password\", \"\")\n",
        "            )\n",
        "            return conn\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to setup PostgreSQL: {e}\")\n",
        "        return None\n",
        "\n",
        "    def setup_mongodb(self):\n",
        "        \"\"\"Setup MongoDB with vector search\"\"\"\n",
        "        try:\n",
        "            params = self.config[\"vector_database\"][\"connection_params\"]\n",
        "            client = MongoClient(params.get(\"connection_string\", \"mongodb://localhost:27017/\"))\n",
        "            db = client[params.get(\"database\", \"rag_db\")]\n",
        "            return db\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to setup MongoDB: {e}\")\n",
        "        return None\n",
        "\n",
        "    def setup_routes(self):\n",
        "        \"\"\"Setup Flask routes\"\"\"\n",
        "\n",
        "        @self.app.route('/health', methods=['GET'])\n",
        "        def health_check():\n",
        "            \"\"\"Health check endpoint\"\"\"\n",
        "            return jsonify({\n",
        "                \"status\": \"healthy\",\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"version\": \"1.0.0\"\n",
        "            })\n",
        "\n",
        "        @self.app.route('/chat', methods=['POST'])\n",
        "        def chat():\n",
        "            \"\"\"Main chat endpoint\"\"\"\n",
        "            try:\n",
        "                data = request.get_json()\n",
        "\n",
        "                # Validate input\n",
        "                if not data or 'query' not in data:\n",
        "                    return jsonify({\"error\": \"Missing 'query' in request body\"}), 400\n",
        "\n",
        "                query = data['query']\n",
        "                session_id = data.get('session_id', str(uuid.uuid4()))\n",
        "                llm_provider = data.get('llm_provider', 'openai')\n",
        "                include_sources = data.get('include_sources', True)\n",
        "\n",
        "                # Process query\n",
        "                result = self.process_query(\n",
        "                    query=query,\n",
        "                    session_id=session_id,\n",
        "                    llm_provider=llm_provider,\n",
        "                    include_sources=include_sources\n",
        "                )\n",
        "\n",
        "                return jsonify(result)\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error in chat endpoint: {e}\")\n",
        "                return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "        @self.app.route('/upload', methods=['POST'])\n",
        "        def upload_document():\n",
        "            \"\"\"Upload and process new documents\"\"\"\n",
        "            try:\n",
        "                if 'file' not in request.files:\n",
        "                    return jsonify({\"error\": \"No file uploaded\"}), 400\n",
        "\n",
        "                file = request.files['file']\n",
        "                if file.filename == '':\n",
        "                    return jsonify({\"error\": \"No file selected\"}), 400\n",
        "\n",
        "                # Save uploaded file\n",
        "                upload_dir = \"uploads\"\n",
        "                os.makedirs(upload_dir, exist_ok=True)\n",
        "                file_path = os.path.join(upload_dir, file.filename)\n",
        "                file.save(file_path)\n",
        "\n",
        "                # Process document\n",
        "                self.rag.build_knowledge_base([file_path])\n",
        "\n",
        "                return jsonify({\n",
        "                    \"message\": \"Document uploaded and processed successfully\",\n",
        "                    \"filename\": file.filename,\n",
        "                    \"stats\": self.rag.get_stats()\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error in upload endpoint: {e}\")\n",
        "                return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "        @self.app.route('/stats', methods=['GET'])\n",
        "        def get_stats():\n",
        "            \"\"\"Get system statistics\"\"\"\n",
        "            try:\n",
        "                stats = self.rag.get_stats()\n",
        "                stats[\"llm_providers\"] = list(self.llm_providers.keys())\n",
        "                stats[\"active_sessions\"] = len(self.sessions)\n",
        "                return jsonify(stats)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error in stats endpoint: {e}\")\n",
        "                return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "        @self.app.route('/session/<session_id>', methods=['GET'])\n",
        "        def get_session(session_id):\n",
        "            \"\"\"Get session history\"\"\"\n",
        "            try:\n",
        "                session = self.sessions.get(session_id, {\"history\": []})\n",
        "                return jsonify(session)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error in session endpoint: {e}\")\n",
        "                return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "        @self.app.route('/session/<session_id>', methods=['DELETE'])\n",
        "        def clear_session(session_id):\n",
        "            \"\"\"Clear session history\"\"\"\n",
        "            try:\n",
        "                if session_id in self.sessions:\n",
        "                    del self.sessions[session_id]\n",
        "                return jsonify({\"message\": \"Session cleared\"})\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error clearing session: {e}\")\n",
        "                return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "    def process_query(self, query: str, session_id: str, llm_provider: str = \"openai\",\n",
        "                     include_sources: bool = True) -> Dict[str, Any]:\n",
        "        \"\"\"Process a query and return structured response\"\"\"\n",
        "\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        # Retrieve relevant chunks\n",
        "        retrieved_chunks = self.rag.retrieve_relevant_chunks(\n",
        "            query,\n",
        "            k=self.config[\"retrieval_params\"][\"top_k\"]\n",
        "        )\n",
        "\n",
        "        # Filter by similarity threshold\n",
        "        threshold = self.config[\"retrieval_params\"][\"similarity_threshold\"]\n",
        "        filtered_chunks = [\n",
        "            (chunk, score) for chunk, score in retrieved_chunks\n",
        "            if score >= threshold\n",
        "        ]\n",
        "\n",
        "        # Generate response using specified LLM provider\n",
        "        response = self.generate_response_with_provider(\n",
        "            query, filtered_chunks, llm_provider\n",
        "        )\n",
        "\n",
        "        # Update session\n",
        "        if session_id not in self.sessions:\n",
        "            self.sessions[session_id] = {\"history\": [], \"created_at\": start_time.isoformat()}\n",
        "\n",
        "        interaction = {\n",
        "            \"timestamp\": start_time.isoformat(),\n",
        "            \"query\": query,\n",
        "            \"response\": response,\n",
        "            \"llm_provider\": llm_provider,\n",
        "            \"retrieved_chunks\": len(filtered_chunks),\n",
        "            \"processing_time\": (datetime.now() - start_time).total_seconds()\n",
        "        }\n",
        "\n",
        "        self.sessions[session_id][\"history\"].append(interaction)\n",
        "\n",
        "        # Prepare response\n",
        "        result = {\n",
        "            \"session_id\": session_id,\n",
        "            \"query\": query,\n",
        "            \"response\": response,\n",
        "            \"metadata\": {\n",
        "                \"llm_provider\": llm_provider,\n",
        "                \"retrieved_chunks\": len(filtered_chunks),\n",
        "                \"processing_time\": interaction[\"processing_time\"],\n",
        "                \"timestamp\": start_time.isoformat()\n",
        "            }\n",
        "        }\n",
        "\n",
        "        if include_sources:\n",
        "            result[\"sources\"] = [\n",
        "                {\n",
        "                    \"text\": chunk.text[:200] + \"...\" if len(chunk.text) > 200 else chunk.text,\n",
        "                    \"source\": chunk.source,\n",
        "                    \"page\": chunk.page_number,\n",
        "                    \"similarity_score\": float(score)\n",
        "                }\n",
        "                for chunk, score in filtered_chunks[:3]  # Top 3 sources\n",
        "            ]\n",
        "\n",
        "        return result\n",
        "\n",
        "    def generate_response_with_provider(self, query: str, retrieved_chunks: List,\n",
        "                                      llm_provider: str) -> str:\n",
        "        \"\"\"Generate response using specified LLM provider\"\"\"\n",
        "\n",
        "        if not retrieved_chunks:\n",
        "            return \"দুঃখিত, আপনার প্রশ্নের উত্তর আমার জ্ঞানভাণ্ডারে খুঁজে পাইনি।\" if self.is_bengali(query) else \"I couldn't find relevant information to answer your question.\"\n",
        "\n",
        "        # Prepare context\n",
        "        context = \"\\n\\n\".join([\n",
        "            f\"[Score: {score:.3f}] {chunk.text}\"\n",
        "            for chunk, score in retrieved_chunks\n",
        "        ])\n",
        "\n",
        "        # Detect query language\n",
        "        is_bengali = self.is_bengali(query)\n",
        "\n",
        "        # System prompt\n",
        "        system_prompt = \"\"\"You are a helpful multilingual assistant specializing in Bengali literature and academic content.\n",
        "        Answer questions based strictly on the provided context.\n",
        "        If the question is in Bengali, answer in Bengali. If in English, answer in English.\n",
        "        Provide direct, accurate answers. If the answer is not in the context, say so politely.\"\"\"\n",
        "\n",
        "        user_prompt = f\"\"\"Context from documents:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Please provide a direct and accurate answer based on the context.\"\"\"\n",
        "\n",
        "        # Try specified provider first\n",
        "        if llm_provider in self.llm_providers:\n",
        "            try:\n",
        "                provider = self.llm_providers[llm_provider]\n",
        "\n",
        "                if llm_provider == \"ollama\":\n",
        "                    # Ollama direct call\n",
        "                    full_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
        "                    response = provider.invoke(full_prompt)\n",
        "                    return response.strip()\n",
        "                else:\n",
        "                    # LangChain providers\n",
        "                    messages = [\n",
        "                        SystemMessage(content=system_prompt),\n",
        "                        HumanMessage(content=user_prompt)\n",
        "                    ]\n",
        "                    response = provider.invoke(messages)\n",
        "                    return response.content.strip()\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error with {llm_provider}: {e}\")\n",
        "\n",
        "        # Fallback to built-in RAG response\n",
        "        return self.rag._simple_response_generation(query, retrieved_chunks, 'bn' if is_bengali else 'en')\n",
        "\n",
        "    def is_bengali(self, text: str) -> bool:\n",
        "        \"\"\"Check if text contains Bengali characters\"\"\"\n",
        "        import re\n",
        "        return bool(re.search(r'[\\u0980-\\u09FF]', text))\n",
        "\n",
        "    def run(self, host='0.0.0.0', port=5000, debug=False):\n",
        "        \"\"\"Run the Flask server\"\"\"\n",
        "        logger.info(f\"Starting RAG API server on {host}:{port}\")\n",
        "        self.app.run(host=host, port=port, debug=debug)\n",
        "\n",
        "# Configuration file generator\n",
        "def generate_config_file():\n",
        "    \"\"\"Generate a sample configuration file\"\"\"\n",
        "    config = {\n",
        "        \"llm_providers\": {\n",
        "            \"openai\": {\n",
        "                \"api_key\": \"your-openai-api-key\",\n",
        "                \"model\": \"gpt-3.5-turbo\"\n",
        "            },\n",
        "            \"google\": {\n",
        "                \"api_key\": \"your-google-api-key\",\n",
        "                \"model\": \"gemini-pro\"\n",
        "            },\n",
        "            \"ollama\": {\n",
        "                \"base_url\": \"http://localhost:11434\",\n",
        "                \"model\": \"llama2\"\n",
        "            }\n",
        "        },\n",
        "        \"vector_database\": {\n",
        "            \"type\": \"faiss\",\n",
        "            \"connection_params\": {\n",
        "                \"pinecone\": {\n",
        "                    \"api_key\": \"your-pinecone-api-key\",\n",
        "                    \"index_name\": \"rag-index\"\n",
        "                },\n",
        "                \"postgres\": {\n",
        "                    \"host\": \"localhost\",\n",
        "                    \"database\": \"rag_db\",\n",
        "                    \"user\": \"postgres\",\n",
        "                    \"password\": \"password\"\n",
        "                },\n",
        "                \"mongodb\": {\n",
        "                    \"connection_string\": \"mongodb://localhost:27017/\",\n",
        "                    \"database\": \"rag_db\"\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        \"vector_store_path\": \"hsc_bangla_vectorstore\",\n",
        "        \"retrieval_params\": {\n",
        "            \"top_k\": 5,\n",
        "            \"similarity_threshold\": 0.3\n",
        "        },\n",
        "        \"generation_params\": {\n",
        "            \"max_tokens\": 300,\n",
        "            \"temperature\": 0.1\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open(\"rag_config.json\", 'w', encoding='utf-8') as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(\"Configuration file 'rag_config.json' generated successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"RAG API Server\")\n",
        "    parser.add_argument(\"--config\", default=\"rag_config.json\", help=\"Configuration file path\")\n",
        "    parser.add_argument(\"--host\", default=\"0.0.0.0\", help=\"Host address\")\n",
        "    parser.add_argument(\"--port\", type=int, default=5000, help=\"Port number\")\n",
        "    parser.add_argument(\"--debug\", action=\"store_true\", help=\"Enable debug mode\")\n",
        "    parser.add_argument(\"--generate-config\", action=\"store_true\", help=\"Generate sample config file\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.generate_config:\n",
        "        generate_config_file()\n",
        "    else:\n",
        "        # Initialize and run API server\n",
        "        api = AdvancedRAGAPI(config_path=args.config)\n",
        "        api.run(host=args.host, port=args.port, debug=args.debug)"
      ],
      "metadata": {
        "id": "0rvlSjHATamv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}